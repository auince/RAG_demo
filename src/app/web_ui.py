import streamlit as st
import sys
import os

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œé˜²æ­¢æ‰¾ä¸åˆ°æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from src.app.chain import build_rag_chain
from src.ingestion.loader import DocLoader
from src.ingestion.cleaner import DataCleaner
from src.ingestion.splitter import HybridSplitter
from src.embedding.vector_db import VectorDBManager

# é¡µé¢é…ç½®
st.set_page_config(page_title="DevDocs RAG", layout="wide")
st.title("ğŸ“š DeepSeek å¼€å‘æ–‡æ¡£åŠ©æ‰‹")

# --- ä¾§è¾¹æ ï¼šæ•°æ®ç®¡ç† ---
with st.sidebar:
    st.header("çŸ¥è¯†åº“ç®¡ç†")
    doc_path = st.text_input("æ–‡æ¡£ç›®å½•è·¯å¾„", value="./data/raw")
    
    if st.button("ğŸ”„ é‡å»ºç´¢å¼• (Rebuild Index)"):
        with st.status("æ­£åœ¨å¤„ç†æ•°æ®...", expanded=True) as status:
            try:
                # 1. åŠ è½½
                st.write("ğŸ“‚ åŠ è½½æ–‡æ¡£...")
                loader = DocLoader(doc_path)
                raw_docs = loader.load()
                
                # 2. æ¸…æ´—
                st.write("ğŸ§¹ æ¸…æ´—æ•°æ®...")
                cleaned_docs = DataCleaner.clean_documents(raw_docs)
                
                # 3. åˆ†å—
                st.write("âœ‚ï¸ æ™ºèƒ½åˆ†å—...")
                splitter = HybridSplitter()
                chunks = splitter.split(cleaned_docs)
                
                # 4. å‘é‡åŒ–
                st.write("ğŸ§  å‘é‡åŒ–å¹¶å­˜å‚¨ (è¿™å¯èƒ½éœ€è¦ä¸€ä¼š)...")
                db_manager = VectorDBManager()
                db_manager.create_index(chunks, force_rebuild=True)
                
                status.update(label="âœ… ç´¢å¼•æ„å»ºå®Œæˆ!", state="complete", expanded=False)
                st.success(f"æˆåŠŸå¤„ç† {len(chunks)} ä¸ªç‰‡æ®µã€‚")
            except Exception as e:
                st.error(f"å‡ºé”™: {str(e)}")

# --- ä¸»ç•Œé¢ï¼šèŠå¤© ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ¡†æ¶çš„ API?"):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # åˆå§‹åŒ–é“¾ (å»ºè®®ç»“åˆ st.cache_resource ä¼˜åŒ–åŠ è½½é€Ÿåº¦)
            chain = build_rag_chain()
            
            # æµå¼è¾“å‡º
            chunks = chain.stream(prompt)
            for chunk in chunks:
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            full_response = f"Error: {e}"

    st.session_state.messages.append({"role": "assistant", "content": full_response})