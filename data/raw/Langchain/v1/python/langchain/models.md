# æ¨¡å‹

[å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰](https://en.wikipedia.org/wiki/Large_language_model) æ˜¯å¼ºå¤§çš„ AI å·¥å…·ï¼Œèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ç†è§£å’Œç”Ÿæˆæ–‡æœ¬ã€‚å®ƒä»¬ç”¨é€”å¹¿æ³›ï¼Œå¯ç”¨äºæ’°å†™å†…å®¹ã€ç¿»è¯‘è¯­è¨€ã€æ€»ç»“ä¿¡æ¯å’Œå›ç­”é—®é¢˜ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯é¡¹ä»»åŠ¡è¿›è¡Œä¸“é—¨è®­ç»ƒã€‚

é™¤äº†æ–‡æœ¬ç”Ÿæˆä¹‹å¤–ï¼Œè®¸å¤šæ¨¡å‹è¿˜æ”¯æŒï¼š

*   **å·¥å…·è°ƒç”¨** - è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æ•°æ®åº“æŸ¥è¯¢æˆ– API è°ƒç”¨ï¼‰å¹¶å°†ç»“æœç”¨äºå“åº”ä¸­ã€‚
*   **ç»“æ„åŒ–è¾“å‡º** - æ¨¡å‹çš„å“åº”è¢«é™åˆ¶ä¸ºéµå¾ªå®šä¹‰çš„æ ¼å¼ã€‚
*   **å¤šæ¨¡æ€** - å¤„ç†å’Œè¿”å›éæ–‡æœ¬æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚
*   **æ¨ç†** - æ¨¡å‹æ‰§è¡Œå¤šæ­¥æ¨ç†ä»¥å¾—å‡ºç»“è®ºã€‚

æ¨¡å‹æ˜¯ [ä»£ç†](agents.html) çš„æ¨ç†å¼•æ“ã€‚å®ƒä»¬é©±åŠ¨ä»£ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œå†³å®šè°ƒç”¨å“ªäº›å·¥å…·ã€å¦‚ä½•è§£é‡Šç»“æœä»¥åŠä½•æ—¶æä¾›æœ€ç»ˆç­”æ¡ˆã€‚

æ‚¨é€‰æ‹©çš„æ¨¡å‹çš„è´¨é‡å’Œèƒ½åŠ›ç›´æ¥å½±å“ä»£ç†çš„å¯é æ€§å’Œæ€§èƒ½ã€‚ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²â€”â€”æœ‰äº›æ›´æ“…é•¿éµå¾ªå¤æ‚æŒ‡ä»¤ï¼Œæœ‰äº›æ›´æ“…é•¿ç»“æ„åŒ–æ¨ç†ï¼Œæœ‰äº›æ”¯æŒæ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ä»¥å¤„ç†æ›´å¤šä¿¡æ¯ã€‚

LangChain çš„æ ‡å‡†æ¨¡å‹æ¥å£ä¸ºæ‚¨æä¾›å¯¹ä¼—å¤šä¸åŒæä¾›å•†é›†æˆçš„è®¿é—®ï¼Œè¿™ä½¿å¾—å®éªŒå’Œåˆ‡æ¢æ¨¡å‹ä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨ç”¨ä¾‹çš„æ¨¡å‹å˜å¾—éå¸¸å®¹æ˜“ã€‚

> **ä¿¡æ¯**
> æœ‰å…³ç‰¹å®šæä¾›å•†çš„é›†æˆä¿¡æ¯å’ŒåŠŸèƒ½ï¼Œè¯·å‚é˜…æä¾›å•†çš„[é›†æˆé¡µé¢](../integrations/providers/overview.html)ã€‚

## åŸºæœ¬ç”¨æ³•

æ¨¡å‹å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼ä½¿ç”¨ï¼š

1.  **ä¸ä»£ç†ä¸€èµ·ä½¿ç”¨** - åˆ›å»º[ä»£ç†](agents.html#model)æ—¶å¯åŠ¨æ€æŒ‡å®šæ¨¡å‹ã€‚
2.  **ç‹¬ç«‹ä½¿ç”¨** - æ¨¡å‹å¯ç›´æ¥è°ƒç”¨ï¼ˆåœ¨ä»£ç†å¾ªç¯ä¹‹å¤–ï¼‰ï¼Œç”¨äºæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»æˆ–æå–ç­‰ä»»åŠ¡ï¼Œè€Œæ— éœ€ä»£ç†æ¡†æ¶ã€‚

åŒä¸€æ¨¡å‹æ¥å£åœ¨ä¸¤ç§ä¸Šä¸‹æ–‡ä¸­å‡é€‚ç”¨ï¼Œè¿™ä¸ºæ‚¨æä¾›äº†ä»ç®€å•å¼€å§‹å¹¶æ ¹æ®éœ€è¦æ‰©å±•åˆ°æ›´å¤æ‚åŸºäºä»£ç†çš„å·¥ä½œæµç¨‹çš„çµæ´»æ€§ã€‚

### åˆå§‹åŒ–æ¨¡å‹

åœ¨ LangChain ä¸­å¼€å§‹ä½¿ç”¨ç‹¬ç«‹æ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) ä»æ‚¨é€‰æ‹©çš„[æä¾›å•†](../integrations/providers/overview.html)åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼ˆä»¥ä¸‹ç¤ºä¾‹ï¼‰ï¼š

---

### OpenAI

ğŸ‘‰ é˜…è¯» [OpenAI èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£](https://langchain-doc.cn/v1/python/integrations/chat/openai/)

```shell
pip install -U "langchain[openai]"
```

```python
import os
from langchain.chat_models import init_chat_model
os.environ["OPENAI_API_KEY"] = "sk-..."
model = init_chat_model("openai:gpt-4.1")
```

```python
import os
from langchain_openai import ChatOpenAI
os.environ["OPENAI_API_KEY"] = "sk-..."
model = ChatOpenAI(model="gpt-4.1")
```

---

### Anthropic

ğŸ‘‰ é˜…è¯» [Anthropic èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£](https://langchain-doc.cn/v1/python/integrations/chat/anthropic/)

```shell
pip install -U "langchain[anthropic]"
```

```python
import os
from langchain.chat_models import init_chat_model
os.environ["ANTHROPIC_API_KEY"] = "sk-..."
model = init_chat_model("anthropic:claude-sonnet-4-5")
```

```python
import os
from langchain_anthropic import ChatAnthropic
os.environ["ANTHROPIC_API_KEY"] = "sk-..."
model = ChatAnthropic(model="claude-sonnet-4-5")
```

---

### Azure

ğŸ‘‰ é˜…è¯» [Azure èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£](https://langchain-doc.cn/v1/python/integrations/chat/azure_chat_openai/)

```shell
pip install -U "langchain[openai]"
```

```python
import os
from langchain.chat_models import init_chat_model
os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"
model = init_chat_model(
    "azure_openai:gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)
```

```python
import os
from langchain_openai import AzureChatOpenAI
os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"
model = AzureChatOpenAI(
    model="gpt-4.1",
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]
)
```

---

### Google Gemini

ğŸ‘‰ é˜…è¯» [Google GenAI èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£](https://langchain-doc.cn/v1/python/integrations/chat/google_generative_ai/)

```shell
pip install -U "langchain[google-genai]"
```

```python
import os
from langchain.chat_models import init_chat_model
os.environ["GOOGLE_API_KEY"] = "..."
model = init_chat_model("google_genai:gemini-2.5-flash-lite")
```

```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI
os.environ["GOOGLE_API_KEY"] = "..."
model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")
```

---

### AWS Bedrock

ğŸ‘‰ é˜…è¯» [AWS Bedrock èŠå¤©æ¨¡å‹é›†æˆæ–‡æ¡£](https://langchain-doc.cn/v1/python/integrations/chat/bedrock/)

```shell
pip install -U "langchain[aws]"
```

```python
from langchain.chat_models import init_chat_model
# è¯·æŒ‰ç…§æ­¤å¤„çš„æ­¥éª¤é…ç½®æ‚¨çš„å‡­æ®ï¼š
# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html
model = init_chat_model(
    "anthropic.claude-3-5-sonnet-20240620-v1:0",
    model_provider="bedrock_converse",
)
```

```python
from langchain_aws import ChatBedrock
model = ChatBedrock(model="anthropic.claude-3-5-sonnet-20240620-v1:0")
```

---

```python
response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰ä¼šè¯´è¯ï¼Ÿ")
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¦‚ä½•ä¼ é€’æ¨¡å‹[å‚æ•°](models.html#%E5%8F%82%E6%95%B0)ï¼Œè¯·å‚é˜… [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model)ã€‚

### å…³é”®æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
| :--- | :--- |
| **Invoke** | æ¨¡å‹æ¥å—æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨ç”Ÿæˆå®Œæ•´å“åº”åè¾“å‡ºæ¶ˆæ¯ã€‚ |
| **Stream** | è°ƒç”¨æ¨¡å‹ï¼Œä½†å®æ—¶æµå¼ä¼ è¾“ç”Ÿæˆçš„è¾“å‡ºã€‚ |
| **Batch** | å°†å¤šä¸ªè¯·æ±‚æ‰¹é‡å‘é€ç»™æ¨¡å‹ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å¤„ç†ã€‚ |

> **ä¿¡æ¯**
> é™¤äº†èŠå¤©æ¨¡å‹ä¹‹å¤–ï¼ŒLangChain è¿˜æ”¯æŒå…¶ä»–ç›¸å…³æŠ€æœ¯ï¼Œå¦‚åµŒå…¥æ¨¡å‹å’Œå‘é‡å­˜å‚¨ã€‚è¯¦æƒ…è¯·å‚é˜…[é›†æˆé¡µé¢](../integrations/providers/overview.html)ã€‚

## å‚æ•°

èŠå¤©æ¨¡å‹æ¥å—å¯ç”¨äºé…ç½®å…¶è¡Œä¸ºçš„ä¸€ç»„å‚æ•°ã€‚æ”¯æŒçš„å‚æ•°é›†å› æ¨¡å‹å’Œæä¾›å•†è€Œå¼‚ï¼Œä½†æ ‡å‡†å‚æ•°åŒ…æ‹¬ï¼š

| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
| :--- | :--- | :--- | :--- |
| `model` | string | æ˜¯ | æ‚¨æƒ³ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹çš„åç§°æˆ–æ ‡è¯†ç¬¦ã€‚ |
| `api_key` | string | å¦ | ç”¨äºå‘æ¨¡å‹æä¾›å•†è¿›è¡Œèº«ä»½éªŒè¯çš„å¯†é’¥ã€‚é€šå¸¸åœ¨æ³¨å†Œè®¿é—®æ¨¡å‹æ—¶é¢å‘ã€‚é€šå¸¸é€šè¿‡è®¾ç½®**ç¯å¢ƒå˜é‡**è®¿é—®ã€‚ |
| `temperature` | number | å¦ | æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ã€‚å€¼è¶Šé«˜ï¼Œå“åº”è¶Šå…·åˆ›é€ æ€§ï¼›å€¼è¶Šä½ï¼Œå“åº”è¶Šç¡®å®šæ€§ã€‚ |
| `timeout` | number | å¦ | åœ¨å–æ¶ˆè¯·æ±‚ä¹‹å‰ç­‰å¾…æ¨¡å‹å“åº”çš„æœ€å¤§æ—¶é—´ï¼ˆç§’ï¼‰ã€‚ |
| `max_tokens` | number | å¦ | é™åˆ¶å“åº”ä¸­çš„**ä»¤ç‰Œ**æ€»æ•°ï¼Œæœ‰æ•ˆæ§åˆ¶è¾“å‡ºé•¿åº¦ã€‚ |
| `max_retries` | number | å¦ | å¦‚æœå› ç½‘ç»œè¶…æ—¶æˆ–é€Ÿç‡é™åˆ¶ç­‰é—®é¢˜è€Œå¤±è´¥ï¼Œç³»ç»Ÿå°†é‡æ–°å‘é€è¯·æ±‚çš„æœ€å¤§å°è¯•æ¬¡æ•°ã€‚ |

ä½¿ç”¨ [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model)ï¼Œå°†è¿™äº›å‚æ•°ä½œä¸ºå†…è” `**kwargs` ä¼ é€’ï¼š

```python
model = init_chat_model(
    "anthropic:claude-sonnet-4-5",
    # ä¼ é€’ç»™æ¨¡å‹çš„ Kwargsï¼š
    temperature=0.7,
    timeout=30,
    max_tokens=1000,
)
```

> **ä¿¡æ¯**
> æ¯ä¸ªèŠå¤©æ¨¡å‹é›†æˆå¯èƒ½å…·æœ‰ç”¨äºæ§åˆ¶æä¾›å•†ç‰¹å®šåŠŸèƒ½çš„é¢å¤–å‚æ•°ã€‚ä¾‹å¦‚ï¼Œ[`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) å…·æœ‰ `use_responses_api` ä»¥å†³å®šæ˜¯å¦ä½¿ç”¨ OpenAI Responses æˆ– Completions APIã€‚
> è¦æŸ¥æ‰¾ç»™å®šèŠå¤©æ¨¡å‹æ”¯æŒçš„æ‰€æœ‰å‚æ•°ï¼Œè¯·è½¬åˆ°[èŠå¤©æ¨¡å‹é›†æˆ](../integrations/chat.html)é¡µé¢ã€‚

---

## è°ƒç”¨

å¿…é¡»è°ƒç”¨èŠå¤©æ¨¡å‹ä»¥ç”Ÿæˆè¾“å‡ºã€‚ä¸»è¦æœ‰ä¸‰ç§è°ƒç”¨æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•é€‚ç”¨äºä¸åŒçš„ç”¨ä¾‹ã€‚

### Invoke

è°ƒç”¨æ¨¡å‹çš„æœ€ç›´æ¥æ–¹æ³•æ˜¯ä½¿ç”¨ [`invoke()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.invoke) ä¼ å…¥å•ä¸ªæ¶ˆæ¯æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚

```python
response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ")
print(response)
```

å¯ä»¥å‘æ¨¡å‹æä¾›æ¶ˆæ¯åˆ—è¡¨ä»¥è¡¨ç¤ºå¯¹è¯å†å²ã€‚æ¯æ¡æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªè§’è‰²ï¼Œæ¨¡å‹ç”¨å®ƒæ¥æŒ‡ç¤ºå¯¹è¯ä¸­è°å‘é€äº†æ¶ˆæ¯ã€‚æœ‰å…³è§’è‰²ã€ç±»å‹å’Œå†…å®¹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ¶ˆæ¯](messages.html)æŒ‡å—ã€‚

```python
from langchain.messages import HumanMessage, AIMessage, SystemMessage
conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„æœ‰ç”¨åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"},
    {"role": "assistant", "content": "J'adore la programmation."},
    {"role": "user", "content": "ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚"}
]
response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applications.")
```

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
conversation = [
    SystemMessage("ä½ æ˜¯ä¸€ä¸ªå°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„æœ‰ç”¨åŠ©æ‰‹ã€‚"),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"),
    AIMessage("J'adore la programmationã€‚"),
    HumanMessage("ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åºã€‚")
]
response = model.invoke(conversation)
print(response)  # AIMessage("J'adore crÃ©er des applicationsã€‚")
```

### Stream

å¤§å¤šæ•°æ¨¡å‹å¯ä»¥åœ¨ç”Ÿæˆæ—¶æµå¼ä¼ è¾“å…¶è¾“å‡ºå†…å®¹ã€‚é€šè¿‡é€æ­¥æ˜¾ç¤ºè¾“å‡ºï¼Œæµå¼ä¼ è¾“æ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·ä½“éªŒï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒé•¿çš„å“åº”ã€‚

è°ƒç”¨ [`stream()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.stream) è¿”å›ä¸€ä¸ª**è¿­ä»£å™¨**ï¼Œå®ƒåœ¨ç”Ÿæˆæ—¶é€å—äº§ç”Ÿè¾“å‡ºã€‚æ‚¨å¯ä»¥ä½¿ç”¨å¾ªç¯å®æ—¶å¤„ç†æ¯ä¸ªå—ï¼š

```python
for chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    print(chunk.text, end="|", flush=True)
```

```python
for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    for block in chunk.content_blocks:
        if block["type"] == "reasoning" and (reasoning := block.get("reasoning")):
            print(f"æ¨ç†ï¼š{reasoning}")
        elif block["type"] == "tool_call_chunk":
            print(f"å·¥å…·è°ƒç”¨å—ï¼š{block}")
        elif block["type"] == "text":
            print(block["text"])
        else:
            ...
```

ä¸è¿”å›å•ä¸ª [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) çš„ [`invoke()`](models.html#invoke) ä¸åŒï¼Œ`stream()` è¿”å›å¤šä¸ª [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) å¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ä¸€éƒ¨åˆ†è¾“å‡ºæ–‡æœ¬ã€‚é‡è¦çš„æ˜¯ï¼Œæµä¸­çš„æ¯ä¸ªå—éƒ½è®¾è®¡ä¸ºé€šè¿‡æ±‚å’Œèšåˆæˆå®Œæ•´æ¶ˆæ¯ï¼š

```python
full = None  # None | AIMessageChunk
for chunk in model.stream("å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"):
    full = chunk if full is None else full + chunk
    print(full.text)
# å¤©ç©º
# å¤©ç©ºæ˜¯
# å¤©ç©ºé€šå¸¸
# å¤©ç©ºé€šå¸¸æ˜¯è“è‰²
# ...
print(full.content_blocks)
# [{"type": "text", "text": "å¤©ç©ºé€šå¸¸æ˜¯è“è‰²..."}]
```

ç”Ÿæˆçš„æ¶ˆæ¯å¯ä»¥ä¸ä½¿ç”¨ [`invoke()`](models.html#invoke) ç”Ÿæˆçš„æ¶ˆæ¯ç›¸åŒå¤„ç†â€”â€”ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥èšåˆæˆæ¶ˆæ¯å†å²å¹¶ä½œä¸ºå¯¹è¯ä¸Šä¸‹æ–‡ä¼ é€’å›æ¨¡å‹ã€‚

> **è­¦å‘Š**
> æµå¼ä¼ è¾“ä»…åœ¨ç¨‹åºçš„æ‰€æœ‰æ­¥éª¤éƒ½çŸ¥é“å¦‚ä½•å¤„ç†å—æµæ—¶æ‰æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œæ— æ³•æµå¼ä¼ è¾“çš„åº”ç”¨ç¨‹åºæ˜¯éœ€è¦å…ˆå°†æ•´ä¸ªè¾“å‡ºå­˜å‚¨åœ¨å†…å­˜ä¸­æ‰èƒ½å¤„ç†çš„æƒ…å†µã€‚

#### é«˜çº§æµå¼ä¼ è¾“ä¸»é¢˜

##### â€œè‡ªåŠ¨æµå¼ä¼ è¾“â€èŠå¤©æ¨¡å‹

LangChain é€šè¿‡åœ¨æŸäº›æƒ…å†µä¸‹è‡ªåŠ¨å¯ç”¨æµå¼ä¼ è¾“æ¨¡å¼æ¥ç®€åŒ–ä»èŠå¤©æ¨¡å‹è¿›è¡Œæµå¼ä¼ è¾“ï¼Œå³ä½¿æ‚¨æœªæ˜¾å¼è°ƒç”¨æµå¼ä¼ è¾“æ–¹æ³•ã€‚è¿™åœ¨æ‚¨ä½¿ç”¨éæµå¼ä¼ è¾“çš„ invoke æ–¹æ³•ä½†ä»å¸Œæœ›æµå¼ä¼ è¾“æ•´ä¸ªåº”ç”¨ç¨‹åºï¼ˆåŒ…æ‹¬èŠå¤©æ¨¡å‹çš„ä¸­é—´ç»“æœï¼‰æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚

ä¾‹å¦‚ï¼Œåœ¨ [LangGraph ä»£ç†](agents.html) ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨èŠ‚ç‚¹å†…è°ƒç”¨ `model.invoke()`ï¼Œä½†å¦‚æœåœ¨æµå¼ä¼ è¾“æ¨¡å¼ä¸‹è¿è¡Œï¼ŒLangChain å°†è‡ªåŠ¨å§”æ‰˜ç»™æµå¼ä¼ è¾“ã€‚

**å·¥ä½œåŸç†**

å½“æ‚¨ `invoke()` ä¸€ä¸ªèŠå¤©æ¨¡å‹æ—¶ï¼Œå¦‚æœ LangChain æ£€æµ‹åˆ°æ‚¨æ­£åœ¨å°è¯•æµå¼ä¼ è¾“æ•´ä¸ªåº”ç”¨ç¨‹åºï¼Œå®ƒå°†è‡ªåŠ¨åˆ‡æ¢åˆ°å†…éƒ¨æµå¼ä¼ è¾“æ¨¡å¼ã€‚å¯¹äºä½¿ç”¨ invoke çš„ä»£ç ï¼Œç»“æœæ˜¯ç›¸åŒçš„ï¼›ç„¶è€Œï¼Œåœ¨èŠå¤©æ¨¡å‹æµå¼ä¼ è¾“æ—¶ï¼ŒLangChain å°†è´Ÿè´£åœ¨ LangChain çš„å›è°ƒç³»ç»Ÿä¸­è°ƒç”¨ [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) äº‹ä»¶ã€‚

å›è°ƒäº‹ä»¶å…è®¸ LangGraph çš„ `stream()` å’Œ [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) å®æ—¶æ˜¾ç¤ºèŠå¤©æ¨¡å‹çš„è¾“å‡ºã€‚

##### æµå¼ä¼ è¾“äº‹ä»¶

LangChain èŠå¤©æ¨¡å‹è¿˜å¯ä»¥ä½¿ç”¨ [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) æµå¼ä¼ è¾“è¯­ä¹‰äº‹ä»¶ã€‚

è¿™ç®€åŒ–äº†åŸºäºäº‹ä»¶ç±»å‹å’Œå…¶ä»–å…ƒæ•°æ®çš„è¿‡æ»¤ï¼Œå¹¶åœ¨åå°èšåˆå®Œæ•´æ¶ˆæ¯ã€‚è¯·å‚é˜…ä»¥ä¸‹ç¤ºä¾‹ã€‚

```python
async for event in model.astream_events("ä½ å¥½"):
    if event["event"] == "on_chat_model_start":
        print(f"è¾“å…¥ï¼š{event['data']['input']}")
    elif event["event"] == "on_chat_model_stream":
        print(f"ä»¤ç‰Œï¼š{event['data']['chunk'].text}")
    elif event["event"] == "on_chat_model_end":
        print(f"å®Œæ•´æ¶ˆæ¯ï¼š{event['data']['output'].text}")
    else:
        pass
```

```txt
è¾“å…¥ï¼šä½ å¥½
ä»¤ç‰Œï¼šä½ 
ä»¤ç‰Œï¼šå¥½
ä»¤ç‰Œï¼šï¼
ä»¤ç‰Œï¼šæˆ‘
ä»¤ç‰Œï¼šèƒ½
ä»¤ç‰Œï¼šå¦‚
...
å®Œæ•´æ¶ˆæ¯ï¼šä½ å¥½ï¼ä»Šå¤©æˆ‘èƒ½å¦‚ä½•å¸®åŠ©æ‚¨ï¼Ÿ
```

> **æç¤º**
> è¯·å‚é˜… [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) å‚è€ƒï¼Œäº†è§£äº‹ä»¶ç±»å‹å’Œå…¶ä»–è¯¦ç»†ä¿¡æ¯ã€‚

### Batch

å°†ä¸€ç»„ç‹¬ç«‹è¯·æ±‚æ‰¹é‡å¤„ç†ç»™æ¨¡å‹å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½å¹¶é™ä½æˆæœ¬ï¼Œå› ä¸ºå¤„ç†å¯ä»¥å¹¶è¡Œè¿›è¡Œï¼š

```python
responses = model.batch([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
])
for response in responses:
    print(response)
```

> **æ³¨æ„**
> æœ¬èŠ‚æè¿°çš„æ˜¯èŠå¤©æ¨¡å‹æ–¹æ³• [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch)ï¼Œå®ƒåœ¨å®¢æˆ·ç«¯å¹¶è¡ŒåŒ–æ¨¡å‹è°ƒç”¨ã€‚
> å®ƒ**ä¸åŒäº**æ¨ç†æä¾›å•†æ”¯æŒçš„æ‰¹é‡ APIï¼Œä¾‹å¦‚ [OpenAI](https://platform.openai.com/docs/guides/batch) æˆ– [Anthropic](https://docs.claude.com/en/docs/build-with-claude/batch-processing#message-batches-api)ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) ä»…è¿”å›æ•´ä¸ªæ‰¹æ¬¡çš„æœ€ç»ˆè¾“å‡ºã€‚å¦‚æœæ‚¨å¸Œæœ›åœ¨æ¯ä¸ªå•ç‹¬è¾“å…¥å®Œæˆç”Ÿæˆæ—¶æ¥æ”¶è¾“å‡ºï¼Œå¯ä»¥ä½¿ç”¨ [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed) æµå¼ä¼ è¾“ç»“æœï¼š

```python
for response in model.batch_as_completed([
    "ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ",
    "é£æœºæ˜¯å¦‚ä½•é£è¡Œçš„ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
]):
    print(response)
```

> **æ³¨æ„**
> ä½¿ç”¨ [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed) æ—¶ï¼Œç»“æœå¯èƒ½æ— åºåˆ°è¾¾ã€‚æ¯ä¸ªç»“æœåŒ…æ‹¬è¾“å…¥ç´¢å¼•ï¼Œä»¥ä¾¿æ ¹æ®éœ€è¦åŒ¹é…å’Œé‡å»ºåŸå§‹é¡ºåºã€‚

> **æç¤º**
> åœ¨ä½¿ç”¨ [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) æˆ– [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed) å¤„ç†å¤§é‡è¾“å…¥æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ§åˆ¶æœ€å¤§å¹¶è¡Œè°ƒç”¨æ•°ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨ [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) å­—å…¸ä¸­è®¾ç½® [`max_concurrency`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig.max_concurrency) å±æ€§æ¥å®Œæˆã€‚

```python
model.batch(
    list_of_inputs,
    config={
        'max_concurrency': 5,  # é™åˆ¶ä¸º 5 ä¸ªå¹¶è¡Œè°ƒç”¨
    }
)
```

æœ‰å…³æ‰¹å¤„ç†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[å‚è€ƒ](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch)ã€‚

---

## å·¥å…·è°ƒç”¨

æ¨¡å‹å¯ä»¥è¯·æ±‚è°ƒç”¨æ‰§è¡Œä»»åŠ¡çš„å·¥å…·ï¼Œä¾‹å¦‚ä»æ•°æ®åº“è·å–æ•°æ®ã€æœç´¢ç½‘ç»œæˆ–è¿è¡Œä»£ç ã€‚å·¥å…·æ˜¯ä»¥ä¸‹å†…å®¹çš„é…å¯¹ï¼š

1.  æ¶æ„ï¼ŒåŒ…æ‹¬å·¥å…·çš„åç§°ã€æè¿°å’Œ/æˆ–å‚æ•°å®šä¹‰ï¼ˆé€šå¸¸æ˜¯ JSON æ¶æ„ï¼‰
2.  è¦æ‰§è¡Œçš„å‡½æ•°æˆ–**åç¨‹**

> **æ³¨æ„**
> æ‚¨å¯èƒ½ä¼šå¬åˆ°â€œå‡½æ•°è°ƒç”¨â€ä¸€è¯ã€‚æˆ‘ä»¬å°†æ­¤ä¸â€œå·¥å…·è°ƒç”¨â€äº’æ¢ä½¿ç”¨ã€‚

è¦ä½¿æ‚¨å®šä¹‰çš„å·¥å…·å¯ä¾›æ¨¡å‹ä½¿ç”¨ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ [`bind_tools()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.bind_tools) ç»‘å®šå®ƒä»¬ã€‚åœ¨åç»­è°ƒç”¨ä¸­ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©è°ƒç”¨ä»»ä½•ç»‘å®šçš„å·¥å…·ã€‚

ä¸€äº›æ¨¡å‹æä¾›å•†æä¾›å†…ç½®å·¥å…·ï¼Œå¯é€šè¿‡æ¨¡å‹æˆ–è°ƒç”¨å‚æ•°å¯ç”¨ï¼ˆä¾‹å¦‚ [`ChatOpenAI`](https://langchain-doc.cn/v1/python/integrations/chat/openai)ã€[`ChatAnthropic`](https://langchain-doc.cn/v1/python/integrations/chat/anthropic)ï¼‰ã€‚è¯·æŸ¥çœ‹ç›¸åº”çš„[æä¾›å•†å‚è€ƒ](../integrations/providers/overview.html)ä»¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

> **æç¤º**
> æœ‰å…³åˆ›å»ºå·¥å…·çš„è¯¦ç»†ä¿¡æ¯å’Œå…¶ä»–é€‰é¡¹ï¼Œè¯·å‚é˜…[å·¥å…·æŒ‡å—](tools.html)ã€‚

```python
from langchain.tools import tool
@tool
def get_weather(location: str) -> str:
    """è·å–æŸä¸ªä½ç½®çš„å¤©æ°”ã€‚"""
    return f"{location} å¤©æ°”æ™´æœ—ã€‚"
model_with_tools = model.bind_tools([get_weather])  # [!code highlight]
response = model_with_tools.invoke("æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
for tool_call in response.tool_calls:
    # æŸ¥çœ‹æ¨¡å‹å‘å‡ºçš„å·¥å…·è°ƒç”¨
    print(f"å·¥å…·ï¼š{tool_call['name']}")
    print(f"å‚æ•°ï¼š{tool_call['args']}")
```

ç»‘å®šç”¨æˆ·å®šä¹‰çš„å·¥å…·æ—¶ï¼Œæ¨¡å‹çš„å“åº”åŒ…æ‹¬**è¯·æ±‚**æ‰§è¡Œå·¥å…·ã€‚å½“å°†æ¨¡å‹ä¸[ä»£ç†](agents.html)åˆ†å¼€ä½¿ç”¨æ—¶ï¼Œæ‚¨éœ€è¦æ‰§è¡Œè¯·æ±‚çš„æ“ä½œå¹¶å°†ç»“æœè¿”å›ç»™æ¨¡å‹ä»¥ç”¨äºåç»­æ¨ç†ã€‚è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[ä»£ç†](agents.html)æ—¶ï¼Œä»£ç†å¾ªç¯å°†ä¸ºæ‚¨å¤„ç†å·¥å…·æ‰§è¡Œå¾ªç¯ã€‚

ä¸‹é¢å±•ç¤ºäº†ä¸€äº›ä½¿ç”¨å·¥å…·è°ƒç”¨çš„å¸¸è§æ–¹æ³•ã€‚

### å·¥å…·æ‰§è¡Œå¾ªç¯

å½“æ¨¡å‹è¿”å›å·¥å…·è°ƒç”¨æ—¶ï¼Œæ‚¨éœ€è¦æ‰§è¡Œå·¥å…·å¹¶å°†ç»“æœä¼ é€’å›æ¨¡å‹ã€‚è¿™ä¼šåˆ›å»ºä¸€ä¸ªå¯¹è¯å¾ªç¯ï¼Œæ¨¡å‹å¯ä»¥ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆå…¶æœ€ç»ˆå“åº”ã€‚LangChain åŒ…å«[ä»£ç†](agents.html)æŠ½è±¡æ¥ä¸ºæ‚¨å¤„ç†æ­¤åè°ƒã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼š

```python
# å°†ï¼ˆå¯èƒ½å¤šä¸ªï¼‰å·¥å…·ç»‘å®šåˆ°æ¨¡å‹
model_with_tools = model.bind_tools([get_weather])
# æ­¥éª¤ 1ï¼šæ¨¡å‹ç”Ÿæˆå·¥å…·è°ƒç”¨
messages = [{"role": "user", "content": "æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)
# æ­¥éª¤ 2ï¼šæ‰§è¡Œå·¥å…·å¹¶æ”¶é›†ç»“æœ
for tool_call in ai_msg.tool_calls:
    # ä½¿ç”¨ç”Ÿæˆçš„å‚æ•°æ‰§è¡Œå·¥å…·
    tool_result = get_weather.invoke(tool_call)
    messages.append(tool_result)
# æ­¥éª¤ 3ï¼šå°†ç»“æœä¼ é€’å›æ¨¡å‹ä»¥è·å–æœ€ç»ˆå“åº”
final_response = model_with_tools.invoke(messages)
print(final_response.text)
# "æ³¢å£«é¡¿å½“å‰å¤©æ°”ä¸º 72Â°Fï¼Œæ™´æœ—ã€‚"
```

æ¯ä¸ªç”±å·¥å…·è¿”å›çš„ [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) åŒ…å«ä¸€ä¸ªä¸åŸå§‹å·¥å…·è°ƒç”¨åŒ¹é…çš„ `tool_call_id`ï¼Œå¸®åŠ©æ¨¡å‹å°†ç»“æœä¸è¯·æ±‚ç›¸å…³è”ã€‚

### å¼ºåˆ¶å·¥å…·è°ƒç”¨

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªç”±é€‰æ‹©ä½¿ç”¨å“ªä¸ªç»‘å®šçš„å·¥å…·ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯èƒ½å¸Œæœ›å¼ºåˆ¶é€‰æ‹©å·¥å…·ï¼Œç¡®ä¿æ¨¡å‹ä½¿ç”¨ç‰¹å®šå·¥å…·æˆ–ç»™å®šåˆ—è¡¨ä¸­çš„**ä»»ä½•**å·¥å…·ï¼š

```python
model_with_tools = model.bind_tools([tool_1], tool_choice="any")
```

```python
model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")
```

### å¹¶è¡Œå·¥å…·è°ƒç”¨

è®¸å¤šæ¨¡å‹æ”¯æŒåœ¨é€‚å½“æ—¶å¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…·ã€‚è¿™å…è®¸æ¨¡å‹åŒæ—¶ä»ä¸åŒæ¥æºæ”¶é›†ä¿¡æ¯ã€‚

```python
model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke(
    "æ³¢å£«é¡¿å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
)
# æ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆå¤šä¸ªå·¥å…·è°ƒç”¨
print(response.tool_calls)
# [
#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
# ]
# æ‰§è¡Œæ‰€æœ‰å·¥å…·ï¼ˆå¯ä»¥ä½¿ç”¨ async å¹¶è¡Œæ‰§è¡Œï¼‰
results = []
for tool_call in response.tool_calls:
    if tool_call['name'] == 'get_weather':
        result = get_weather.invoke(tool_call)
    ...
    results.append(result)
```

æ¨¡å‹æ ¹æ®è¯·æ±‚æ“ä½œçš„ç‹¬ç«‹æ€§æ™ºèƒ½åœ°ç¡®å®šä½•æ—¶é€‚åˆå¹¶è¡Œæ‰§è¡Œã€‚

> **æç¤º**
> å¤§å¤šæ•°æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹é»˜è®¤å¯ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨ã€‚æŸäº›æ¨¡å‹ï¼ˆåŒ…æ‹¬ [OpenAI](https://langchain-doc.cn/v1/python/integrations/chat/openai) å’Œ [Anthropic](https://langchain-doc.cn/v1/python/integrations/chat/anthropic)ï¼‰å…è®¸æ‚¨ç¦ç”¨æ­¤åŠŸèƒ½ã€‚è¦æ‰§è¡Œæ­¤æ“ä½œï¼Œè¯·è®¾ç½® `parallel_tool_calls=False`ï¼š

```python
model.bind_tools([get_weather], parallel_tool_calls=False)
```

### æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨

åœ¨æµå¼ä¼ è¾“å“åº”æ—¶ï¼Œå·¥å…·è°ƒç”¨é€šè¿‡ [`ToolCallChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolCallChunk) é€æ­¥æ„å»ºã€‚è¿™å…è®¸æ‚¨åœ¨ç”Ÿæˆå·¥å…·è°ƒç”¨æ—¶æŸ¥çœ‹å®ƒä»¬ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”ã€‚

```python
for chunk in model_with_tools.stream(
    "æ³¢å£«é¡¿å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
):
    # å·¥å…·è°ƒç”¨å—é€æ­¥åˆ°è¾¾
    for tool_chunk in chunk.tool_call_chunks:
        if name := tool_chunk.get("name"):
            print(f"å·¥å…·ï¼š{name}")
        if id_ := tool_chunk.get("id"):
            print(f"IDï¼š{id_}")
        if args := tool_chunk.get("args"):
            print(f"å‚æ•°ï¼š{args}")
# è¾“å‡ºï¼š
# å·¥å…·ï¼šget_weather
# IDï¼šcall_SvMlU1TVIZugrFLckFE2ceRE
# å‚æ•°ï¼š{"lo
# å‚æ•°ï¼šcatio
# å‚æ•°ï¼šn": "B
# å‚æ•°ï¼šosto
# å‚æ•°ï¼šn"}
# å·¥å…·ï¼šget_weather
# IDï¼šcall_QMZdy6qInx13oWKE7KhuhOLR
# å‚æ•°ï¼š{"lo
# å‚æ•°ï¼šcatio
# å‚æ•°ï¼šn": "T
# å‚æ•°ï¼šokyo
# å‚æ•°ï¼š"}
```

æ‚¨å¯ä»¥ç´¯ç§¯å—ä»¥æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼š

```python
gathered = None
for chunk in model_with_tools.stream("æ³¢å£«é¡¿çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"):
    gathered = chunk if gathered is None else gathered + chunk
    print(gathered.tool_calls)
```

---

## ç»“æ„åŒ–è¾“å‡º

å¯ä»¥è¯·æ±‚æ¨¡å‹ä»¥åŒ¹é…ç»™å®šæ¶æ„çš„æ ¼å¼æä¾›å…¶å“åº”ã€‚è¿™å¯¹äºç¡®ä¿è¾“å‡ºæ˜“äºè§£æå¹¶ç”¨äºåç»­å¤„ç†éå¸¸æœ‰ç”¨ã€‚LangChain æ”¯æŒå¤šç§æ¶æ„ç±»å‹å’Œå¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–è¾“å‡ºçš„æ–¹æ³•ã€‚

### Pydantic

[Pydantic æ¨¡å‹](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) æä¾›æœ€ä¸°å¯Œçš„åŠŸèƒ½é›†ï¼ŒåŒ…æ‹¬å­—æ®µéªŒè¯ã€æè¿°å’ŒåµŒå¥—ç»“æ„ã€‚

```python
from pydantic import BaseModel, Field
class Movie(BaseModel):
    """ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: str = Field(..., description="ç”µå½±æ ‡é¢˜")
    year: int = Field(..., description="ç”µå½±ä¸Šæ˜ å¹´ä»½")
    director: str = Field(..., description="ç”µå½±å¯¼æ¼”")
    rating: float = Field(..., description="ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†")
model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)
```

### TypedDict

`TypedDict` æä¾›ä½¿ç”¨ Python å†…ç½®ç±»å‹çš„æ›´ç®€å•æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºä¸éœ€è¦è¿è¡Œæ—¶éªŒè¯çš„æƒ…å†µã€‚

```python
from typing_extensions import TypedDict, Annotated
class MovieDict(TypedDict):
    """ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: Annotated[str, ..., "ç”µå½±æ ‡é¢˜"]
    year: Annotated[int, ..., "ç”µå½±ä¸Šæ˜ å¹´ä»½"]
    director: Annotated[str, ..., "ç”µå½±å¯¼æ¼”"]
    rating: Annotated[float, ..., "ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†"]
model_with_structure = model.with_structured_output(MovieDict)
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}
```

### JSON Schema

ä¸ºäº†è·å¾—æœ€å¤§æ§åˆ¶æˆ–äº’æ“ä½œæ€§ï¼Œæ‚¨å¯ä»¥æä¾›åŸå§‹ JSON æ¶æ„ã€‚

```python
import json
json_schema = {
    "title": "Movie",
    "description": "ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±",
    "type": "object",
    "properties": {
        "title": {
            "type": "string",
            "description": "ç”µå½±æ ‡é¢˜"
        },
        "year": {
            "type": "integer",
            "description": "ç”µå½±ä¸Šæ˜ å¹´ä»½"
        },
        "director": {
            "type": "string",
            "description": "ç”µå½±å¯¼æ¼”"
        },
        "rating": {
            "type": "number",
            "description": "ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†"
        }
    },
    "required": ["title", "year", "director", "rating"]
}
model_with_structure = model.with_structured_output(
    json_schema,
    method="json_schema",
)
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
print(response)  # {'title': 'Inception', 'year': 2010, ...}
```

> **æ³¨æ„**
> **ç»“æ„åŒ–è¾“å‡ºçš„å…³é”®è€ƒè™‘å› ç´ ï¼š**
>
> *   **æ–¹æ³•å‚æ•°**ï¼šæŸäº›æä¾›å•†æ”¯æŒä¸åŒçš„æ–¹æ³•ï¼ˆ`'json_schema'`ã€`'function_calling'`ã€`'json_mode'`ï¼‰
>     *   `'json_schema'` é€šå¸¸æŒ‡æä¾›å•†æä¾›çš„ä¸“ç”¨ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½
>     *   `'function_calling'` é€šè¿‡å¼ºåˆ¶[å·¥å…·è°ƒç”¨](models.html#%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8)éµå¾ªç»™å®šæ¶æ„æ¥æ´¾ç”Ÿç»“æ„åŒ–è¾“å‡º
>     *   `'json_mode'` æ˜¯æŸäº›æä¾›å•†æä¾›çš„ `'json_schema'` çš„å‰èº«â€”â€”å®ƒç”Ÿæˆæœ‰æ•ˆçš„ JSONï¼Œä½†æ¶æ„å¿…é¡»åœ¨æç¤ºä¸­æè¿°
> *   **åŒ…å«åŸå§‹**ï¼šä½¿ç”¨ `include_raw=True` ä»¥åŒæ—¶è·å–å·²è§£æçš„è¾“å‡ºå’ŒåŸå§‹ AI æ¶ˆæ¯
> *   **éªŒè¯**ï¼šPydantic æ¨¡å‹æä¾›è‡ªåŠ¨éªŒè¯ï¼Œè€Œ `TypedDict` å’Œ JSON Schema éœ€è¦æ‰‹åŠ¨éªŒè¯

#### ç¤ºä¾‹ï¼šæ¶ˆæ¯è¾“å‡ºä¸è§£æç»“æ„å¹¶å­˜

è¿”å›åŸå§‹ [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) å¯¹è±¡ä¸è§£æè¡¨ç¤ºä¸€èµ·ä»¥è®¿é—®å“åº”å…ƒæ•°æ®ï¼ˆå¦‚[ä»¤ç‰Œè®¡æ•°](models.html#%E4%BB%A4%E7%89%8C%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5)ï¼‰å¯èƒ½å¾ˆæœ‰ç”¨ã€‚ä¸ºæ­¤ï¼Œåœ¨è°ƒç”¨ [`with_structured_output`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output) æ—¶è®¾ç½® [`include_raw=True`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output(include_raw))ï¼š

```python
from pydantic import BaseModel, Field
class Movie(BaseModel):
    """ä¸€éƒ¨å¸¦æœ‰è¯¦ç»†ä¿¡æ¯çš„ç”µå½±ã€‚"""
    title: str = Field(..., description="ç”µå½±æ ‡é¢˜")
    year: int = Field(..., description="ç”µå½±ä¸Šæ˜ å¹´ä»½")
    director: str = Field(..., description="ç”µå½±å¯¼æ¼”")
    rating: float = Field(..., description="ç”µå½±è¯„åˆ†ï¼Œæ»¡åˆ† 10 åˆ†")
model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]
response = model_with_structure.invoke("æä¾›å…³äºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„è¯¦ç»†ä¿¡æ¯")
response
# {
#     "raw": AIMessage(...),
#     "parsed": Movie(title=..., year=..., ...),
#     "parsing_error": None,
# }
```

#### ç¤ºä¾‹ï¼šåµŒå¥—ç»“æ„

æ¶æ„å¯ä»¥åµŒå¥—ï¼š

```python
from pydantic import BaseModel, Field
class Actor(BaseModel):
    name: str
    role: str
class MovieDetails(BaseModel):
    title: str
    year: int
    cast: list[Actor]
    genres: list[str]
    budget: float | None = Field(None, description="é¢„ç®—ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰")
model_with_structure = model.with_structured_output(MovieDetails)
```

```python
from typing_extensions import Annotated, TypedDict
class Actor(TypedDict):
    name: str
    role: str
class MovieDetails(TypedDict):
    title: str
    year: int
    cast: list[Actor]
    genres: list[str]
    budget: Annotated[float | None, ..., "é¢„ç®—ï¼ˆç™¾ä¸‡ç¾å…ƒï¼‰"]
model_with_structure = model.with_structured_output(MovieDetails)
```

---

## æ”¯æŒçš„æ¨¡å‹

LangChain æ”¯æŒæ‰€æœ‰ä¸»è¦æ¨¡å‹æä¾›å•†ï¼ŒåŒ…æ‹¬ OpenAIã€Anthropicã€Googleã€Azureã€AWS Bedrock ç­‰ã€‚æ¯ä¸ªæä¾›å•†æä¾›å…·æœ‰ä¸åŒåŠŸèƒ½çš„å„ç§æ¨¡å‹ã€‚æœ‰å…³ LangChain ä¸­æ”¯æŒçš„æ¨¡å‹å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜…[é›†æˆé¡µé¢](../integrations/providers/overview.html)ã€‚

---

## é«˜çº§ä¸»é¢˜

### å¤šæ¨¡æ€

æŸäº›æ¨¡å‹å¯ä»¥å¤„ç†å’Œè¿”å›éæ–‡æœ¬æ•°æ®ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚æ‚¨å¯ä»¥é€šè¿‡æä¾›[å†…å®¹å—](messages.html#message-content)å°†éæ–‡æœ¬æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚

> **æç¤º**
> æ‰€æœ‰å…·æœ‰åº•å±‚å¤šæ¨¡æ€åŠŸèƒ½çš„ LangChain èŠå¤©æ¨¡å‹éƒ½æ”¯æŒï¼š
>
> 1.  è·¨æä¾›å•†æ ‡å‡†æ ¼å¼çš„æ•°æ®ï¼ˆè¯·å‚é˜…[æˆ‘ä»¬çš„æ¶ˆæ¯æŒ‡å—](messages.html)ï¼‰
> 2.  OpenAI [èŠå¤©å®Œæˆ](https://platform.openai.com/docs/api-reference/chat)æ ¼å¼
> 3.  ç‰¹å®šäºè¯¥æä¾›å•†çš„ä»»ä½•æ ¼å¼ï¼ˆä¾‹å¦‚ï¼ŒAnthropic æ¨¡å‹æ¥å— Anthropic åŸç”Ÿæ ¼å¼ï¼‰

æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ¶ˆæ¯æŒ‡å—çš„[å¤šæ¨¡æ€éƒ¨åˆ†](messages.html#multimodal)ã€‚

> **æç¤º**
> å¹¶éæ‰€æœ‰ LLM éƒ½æ˜¯å¹³ç­‰çš„ï¼
> æŸäº›æ¨¡å‹å¯ä»¥ä½œä¸ºå…¶å“åº”çš„ä¸€éƒ¨åˆ†è¿”å›å¤šæ¨¡æ€æ•°æ®ã€‚å¦‚æœè°ƒç”¨å®ƒä»¬è¿™æ ·åšï¼Œåˆ™ç”Ÿæˆçš„ [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) å°†å…·æœ‰å¤šæ¨¡æ€ç±»å‹çš„å†…å®¹å—ã€‚

```python
response = model.invoke("åˆ›å»ºä¸€å¼ çŒ«çš„å›¾ç‰‡")
print(response.content_blocks)
# [
#     {"type": "text", "text": "è¿™æ˜¯ä¸€å¼ çŒ«çš„å›¾ç‰‡"},
#     {"type": "image", "base64": "...", "mime_type": "image/jpeg"},
# ]
```

æœ‰å…³ç‰¹å®šæä¾›å•†çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[é›†æˆé¡µé¢](../integrations/providers/overview.html)ã€‚

### æ¨ç†

è¾ƒæ–°çš„æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå¤šæ­¥æ¨ç†ä»¥å¾—å‡ºç»“è®ºã€‚è¿™æ¶‰åŠå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“ç®¡ç†çš„æ­¥éª¤ã€‚

**å¦‚æœåº•å±‚æ¨¡å‹æ”¯æŒ**ï¼Œæ‚¨å¯ä»¥æ˜¾ç¤ºæ­¤æ¨ç†è¿‡ç¨‹ä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹å¦‚ä½•å¾—å‡ºå…¶æœ€ç»ˆç­”æ¡ˆã€‚

```python
for chunk in model.stream("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ"):
    reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
    print(reasoning_steps if reasoning_steps else chunk.text)
```

```python
response = model.invoke("ä¸ºä»€ä¹ˆé¹¦é¹‰æœ‰äº”é¢œå…­è‰²çš„ç¾½æ¯›ï¼Ÿ")
reasoning_steps = [b for b in response.content_blocks if b["type"] == "reasoning"]
print(" ".join(step["reasoning"] for step in reasoning_steps))
```

æ ¹æ®æ¨¡å‹ï¼Œæ‚¨æœ‰æ—¶å¯ä»¥æŒ‡å®šå®ƒåº”æŠ•å…¥æ¨ç†çš„åŠªåŠ›ç¨‹åº¦ã€‚åŒæ ·ï¼Œæ‚¨å¯ä»¥è¯·æ±‚æ¨¡å‹å®Œå…¨å…³é—­æ¨ç†ã€‚è¿™å¯èƒ½é‡‡ç”¨â€œå±‚çº§â€ï¼ˆä¾‹å¦‚ `'low'` æˆ– `'high'`ï¼‰æˆ–æ•´æ•°ä»¤ç‰Œé¢„ç®—çš„å½¢å¼ã€‚

æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[é›†æˆé¡µé¢](../integrations/providers/overview.html)æˆ–æ‚¨ç›¸åº”èŠå¤©æ¨¡å‹çš„[å‚è€ƒ](https://reference.langchain.com/python/integrations/)ã€‚

### æœ¬åœ°æ¨¡å‹

LangChain æ”¯æŒåœ¨æ‚¨è‡ªå·±çš„ç¡¬ä»¶ä¸Šæœ¬åœ°è¿è¡Œæ¨¡å‹ã€‚è¿™å¯¹äºæ•°æ®éšç§è‡³å…³é‡è¦ã€æ‚¨æƒ³è°ƒç”¨è‡ªå®šä¹‰æ¨¡å‹æˆ–å¸Œæœ›é¿å…ä½¿ç”¨åŸºäºäº‘çš„æ¨¡å‹æ‰€äº§ç”Ÿçš„æˆæœ¬çš„åœºæ™¯éå¸¸æœ‰ç”¨ã€‚

[Ollama](https://langchain-doc.cn/v1/python/integrations/chat/ollama) æ˜¯æœ¬åœ°è¿è¡Œæ¨¡å‹çš„æœ€ç®€å•æ–¹æ³•ä¹‹ä¸€ã€‚è¯·å‚é˜…[é›†æˆé¡µé¢](../integrations/providers/overview.html)ä¸Šçš„æœ¬åœ°é›†æˆå®Œæ•´åˆ—è¡¨ã€‚

### æç¤ºç¼“å­˜

è®¸å¤šæä¾›å•†æä¾›æç¤ºç¼“å­˜åŠŸèƒ½ï¼Œä»¥å‡å°‘å¯¹ç›¸åŒä»¤ç‰Œé‡å¤å¤„ç†çš„å»¶è¿Ÿå’Œæˆæœ¬ã€‚è¿™äº›åŠŸèƒ½å¯ä»¥æ˜¯**éšå¼**æˆ–**æ˜¾å¼**ï¼š

*   **éšå¼æç¤ºç¼“å­˜ï¼š** å¦‚æœè¯·æ±‚å‘½ä¸­ç¼“å­˜ï¼Œæä¾›å•†å°†è‡ªåŠ¨ä¼ é€’æˆæœ¬èŠ‚çœã€‚ç¤ºä¾‹ï¼š[OpenAI](https://langchain-doc.cn/v1/python/integrations/chat/openai) å’Œ [Gemini](https://langchain-doc.cn/v1/python/integrations/chat/google_generative_ai)ï¼ˆGemini 2.5 åŠä»¥ä¸Šï¼‰ã€‚
*   **æ˜¾å¼ç¼“å­˜ï¼š** æä¾›å•†å…è®¸æ‚¨æ‰‹åŠ¨æŒ‡ç¤ºç¼“å­˜ç‚¹ä»¥è·å¾—æ›´å¤§æ§åˆ¶æˆ–ä¿è¯æˆæœ¬èŠ‚çœã€‚ç¤ºä¾‹ï¼š[`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/)ï¼ˆé€šè¿‡ `prompt_cache_key`ï¼‰ã€Anthropic çš„ [`AnthropicPromptCachingMiddleware`](https://langchain-doc.cn/v1/python/integrations/chat/anthropic#prompt-caching) å’Œ [`cache_control`](https://docs.langchain.com/v1/python/integrations/chat/anthropic#prom