# æ„å»ºä¸€ä¸ªèŠå¤©æœºå™¨äºº

> æ³¨
> æœ¬æ•™ç¨‹ä¹‹å‰ä½¿ç”¨äº† [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) æŠ½è±¡ã€‚ä½ ä»ç„¶å¯ä»¥åœ¨ [v0.2 æ–‡æ¡£](https://python.langchain.com/v0.2/docs/tutorials/chatbot/) ä¸­è®¿é—®è¯¥ç‰ˆæœ¬çš„å†…å®¹ã€‚
> 
> è‡ª LangChain v0.3 èµ·ï¼Œæˆ‘ä»¬æ¨èç”¨æˆ·ä½¿ç”¨ [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) æ¥åœ¨æ–°çš„ LangChain åº”ç”¨ä¸­é›†æˆ `memory` åŠŸèƒ½ã€‚
> 
> å¦‚æœä½ çš„ä»£ç å·²ç»åœ¨ä½¿ç”¨ `RunnableWithMessageHistory` æˆ– `BaseChatMessageHistory`ï¼Œä½ **æ— éœ€è¿›è¡Œä»»ä½•ä¿®æ”¹**ã€‚æˆ‘ä»¬æš‚æ—¶æ²¡æœ‰è®¡åˆ’å¼ƒç”¨æ­¤åŠŸèƒ½ï¼Œå› ä¸ºå®ƒä»ç„¶éå¸¸é€‚åˆç”¨äºç®€å•èŠå¤©åº”ç”¨ã€‚ä»»ä½•ä½¿ç”¨ `RunnableWithMessageHistory` çš„ä»£ç å°†ç»§ç»­æŒ‰é¢„æœŸå·¥ä½œã€‚
> 
> è¯¦æƒ…è¯·å‚é˜… [å¦‚ä½•è¿ç§»åˆ° LangGraph Memory](https://langchain-doc.cn/versions/migrating_memory/)ã€‚

## æ¦‚è§ˆ

æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥å±•ç¤ºå¦‚ä½•è®¾è®¡å¹¶å®ç°ä¸€ä¸ªç”± LLM é©±åŠ¨çš„èŠå¤©æœºå™¨äººã€‚
è¯¥èŠå¤©æœºå™¨äººèƒ½å¤Ÿä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œå¹¶è®°ä½ä¹‹å‰ä¸ [èŠå¤©æ¨¡å‹](https://langchain-doc.cn/concepts/chat_models) çš„äº¤äº’å†…å®¹ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ„å»ºçš„è¿™ä¸ªèŠå¤©æœºå™¨äººä»…ä½¿ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚
å¦‚æœä½ åœ¨å¯»æ‰¾æ›´é«˜çº§çš„åŠŸèƒ½ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹ç›¸å…³æ¦‚å¿µï¼š

*   [Conversational RAG](https://langchain-doc.cn/tutorials/qa_chat_history)ï¼šè®©èŠå¤©æœºå™¨äººå¯ä»¥åŸºäºå¤–éƒ¨æ•°æ®æºè¿›è¡Œå¯¹è¯
*   [Agents](https://langchain-doc.cn/tutorials/agents)ï¼šæ„å»ºèƒ½æ‰§è¡Œæ“ä½œçš„èŠå¤©æœºå™¨äºº

æœ¬æ•™ç¨‹å°†ä»‹ç»åŸºç¡€éƒ¨åˆ†ï¼Œè¿™å¯¹ç†è§£ä»¥ä¸Šä¸¤ä¸ªé«˜çº§ä¸»é¢˜å¾ˆæœ‰å¸®åŠ©ã€‚ä½†å¦‚æœä½ å·²ç»ç†Ÿæ‚‰ï¼Œå¯ä»¥ç›´æ¥è·³è½¬ã€‚

## è®¾ç½®

### Jupyter Notebook

æœ¬æŒ‡å—ï¼ˆä»¥åŠå¤§å¤šæ•°å…¶ä»–æ–‡æ¡£ï¼‰å‡ä½¿ç”¨ [Jupyter notebooks](https://jupyter.org/)ï¼Œå¹¶å‡è®¾è¯»è€…ä¹Ÿä½¿ç”¨ç›¸åŒç¯å¢ƒã€‚Jupyter notebooks éå¸¸é€‚åˆå­¦ä¹ å¦‚ä½•æ“ä½œ LLM ç³»ç»Ÿï¼Œå› ä¸ºåœ¨å®è·µè¿‡ç¨‹ä¸­ç»å¸¸ä¼šå‡ºç°æ„å¤–ï¼ˆè¾“å‡ºå¼‚å¸¸ã€API æ— å“åº”ç­‰ï¼‰ï¼Œè€Œäº¤äº’å¼ç¯å¢ƒèƒ½å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£ç³»ç»Ÿè¿è¡ŒåŸç†ã€‚

æœ¬æ•™ç¨‹å’Œå…¶ä»–æ•™ç¨‹æœ€æ–¹ä¾¿çš„è¿è¡Œæ–¹å¼éƒ½æ˜¯åœ¨ Jupyter notebook ä¸­å®Œæˆã€‚å®‰è£…æ–¹æ³•è¯·å‚è€ƒ [å®‰è£…æŒ‡å—](https://jupyter.org/install)ã€‚

### å®‰è£…ä¾èµ–

æœ¬æ•™ç¨‹éœ€è¦ä½¿ç”¨ `langchain-core` å’Œ `langgraph`ã€‚è¯·ç¡®ä¿ä½ çš„ `langgraph` ç‰ˆæœ¬ä¸ä½äº `0.2.28`ã€‚

#### Pip

```shell
pip install langchain-core langgraph>0.2.27
```

#### Conda

```shell
conda install langchain-core langgraph>0.2.27 -c conda-forge
```

æ›´å¤šè¯¦æƒ…è¯·å‚é˜… [å®‰è£…æŒ‡å—](../how_to/installation.html)ã€‚

### LangSmith

ä½ åœ¨ LangChain ä¸­æ„å»ºçš„è®¸å¤šåº”ç”¨å¯èƒ½åŒ…å«å¤šä¸ªæ­¥éª¤å’Œå¤šæ¬¡ LLM è°ƒç”¨ã€‚
éšç€åº”ç”¨å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œèƒ½å¤Ÿæ¸…æ™°åœ°æŸ¥çœ‹é“¾æ¡æˆ–ä»£ç†å†…éƒ¨çš„è¿è¡Œè¿‡ç¨‹å°±å˜å¾—å°¤ä¸ºé‡è¦ã€‚
å®ç°è¿™ä¸€ç‚¹çš„æœ€ä½³æ–¹å¼å°±æ˜¯ä½¿ç”¨ [LangSmith](https://smith.langchain.com)ã€‚

æ³¨å†Œè´¦æˆ·åï¼Œ**ï¼ˆè¯·å‰å¾€ LangSmith ç½‘ç«™çš„ Settings -> API Keys é¡µé¢åˆ›å»º API Keyï¼‰**ï¼Œç„¶åè®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä»¥å¯ç”¨æ—¥å¿—è¿½è¸ªï¼š

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

æˆ–è€…ï¼Œåœ¨ Notebook ç¯å¢ƒä¸­å¯ä»¥è¿™æ ·è®¾ç½®ï¼š

```python
import getpass
import os
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## å¿«é€Ÿä¸Šæ‰‹

é¦–å…ˆï¼Œæˆ‘ä»¬å­¦ä¹ å¦‚ä½•å•ç‹¬ä½¿ç”¨è¯­è¨€æ¨¡å‹ã€‚LangChain æ”¯æŒå¤šç§å¯äº’æ¢çš„è¯­è¨€æ¨¡å‹â€”â€”è¯·é€‰æ‹©ä½ æƒ³ä½¿ç”¨çš„æ¨¡å‹ï¼š

é€‰æ‹©ä¼šè¯æ¨¡å‹ï¼š

### OpenAI

```shell
pip install -qU "langchain[openai]"
```

```python
import getpass
import os
if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")
from langchain.chat_models import init_chat_model
model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

### Anthropic

```shell
pip install -qU "langchain[anthropic]"
```

```python
import getpass
import os
if not os.environ.get("ANTHROPIC_API_KEY"):
  os.environ["ANTHROPIC_API_KEY"] = getpass.getpass("Enter API key for Anthropic: ")
# Note: Model versions may become outdated. Check https://docs.anthropic.com/en/docs/about-claude/models/overview for latest versions
from langchain.chat_models import init_chat_model
model = init_chat_model("claude-3-7-sonnet-20250219", model_provider="anthropic")
```

[æ›´æ–°æ¨¡å‹è°ƒç”¨æ–¹æ³•](https://python.langchain.com/docs/tutorials/chatbot/#quickstart)

æˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨è¯¥æ¨¡å‹ã€‚`ChatModel` æ˜¯ LangChain â€œRunnableâ€ çš„å®ä¾‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬æä¾›äº†ç»Ÿä¸€çš„è°ƒç”¨æ¥å£ã€‚è¦ç®€å•è°ƒç”¨æ¨¡å‹ï¼Œåªéœ€å°†æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™ `.invoke` æ–¹æ³•ã€‚

```python
from langchain_core.messages import HumanMessage
model.invoke([HumanMessage(content="Hi! I'm Bob")])
```

ï¼ˆæ­¤å¤„çœç•¥è¾“å‡ºå†…å®¹ï¼‰

å•ç‹¬çš„æ¨¡å‹æ²¡æœ‰ä»»ä½•â€œè®°å¿†â€æˆ–ä¸Šä¸‹æ–‡ã€‚å¦‚æœæˆ‘ä»¬æ¥ç€é—®ä¸€ä¸ªé—®é¢˜ï¼š

```python
model.invoke([HumanMessage(content="What's my name?")])
```

æ¨¡å‹ä¸ä¼šè®°å¾—ä¹‹å‰çš„æ¶ˆæ¯â€”â€”å› ä¸ºå®ƒæ²¡æœ‰ä¿å­˜å¯¹è¯å†å²ã€‚
ä¸ºäº†è®©èŠå¤©æœºå™¨äººâ€œè®°å¾—â€æˆ‘ä»¬è¯´è¿‡çš„è¯ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ•´ä¸ª [å¯¹è¯å†å²](https://langchain-doc.cn/concepts/chat_history) ä¸€å¹¶ä¼ å…¥æ¨¡å‹ã€‚

```python
from langchain_core.messages import AIMessage
model.invoke(
    [
        HumanMessage(content="Hi! I'm Bob"),
        AIMessage(content="Hello Bob! How can I assist you today?"),
        HumanMessage(content="What's my name?"),
    ]
)
```

AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})

---

ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹è¿”å›äº†ä¸€ä¸ªæ­£ç¡®çš„å“åº”ï¼

è¿™å°±æ˜¯èŠå¤©æœºå™¨äººèƒ½å¤Ÿè¿›è¡Œâ€œå¯¹è¯å¼äº¤äº’â€çš„åŸºæœ¬åŸç†ã€‚

é‚£ä¹ˆæˆ‘ä»¬è¯¥å¦‚ä½•æ›´å¥½åœ°å®ç°å®ƒå‘¢ï¼Ÿ

---

## æ¶ˆæ¯æŒä¹…åŒ–ï¼ˆMessage persistenceï¼‰

[LangGraph](https://langchain-ai.github.io/langgraph/) å®ç°äº†ä¸€ä¸ª **å†…ç½®çš„æŒä¹…åŒ–å±‚ï¼ˆpersistence layerï¼‰**ï¼Œè¿™ä½¿å®ƒéå¸¸é€‚åˆç”¨äºæ”¯æŒå¤šè½®å¯¹è¯çš„èŠå¤©åº”ç”¨ç¨‹åºã€‚

å°†æˆ‘ä»¬çš„èŠå¤©æ¨¡å‹å°è£…è¿›ä¸€ä¸ªæœ€å°çš„ LangGraph åº”ç”¨ä¸­ï¼Œå°±å¯ä»¥è‡ªåŠ¨ä¿å­˜æ¶ˆæ¯å†å²ï¼Œä»è€Œç®€åŒ–å¤šè½®å¯¹è¯ç±»åº”ç”¨çš„å¼€å‘ã€‚

LangGraph è‡ªå¸¦ä¸€ä¸ªç®€å•çš„ **å†…å­˜æ£€æŸ¥ç‚¹ç³»ç»Ÿï¼ˆin-memory checkpointerï¼‰**ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­å°†ä½¿ç”¨å®ƒã€‚
è¯¦ç»†å†…å®¹å¯ä»¥æŸ¥çœ‹å…¶[å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/concepts/persistence/)ï¼Œå…¶ä¸­ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨å…¶ä»–æŒä¹…åŒ–åç«¯ï¼ˆä¾‹å¦‚ SQLite æˆ– Postgresï¼‰ã€‚

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
# å®šä¹‰ä¸€ä¸ªæ–°çš„å›¾
workflow = StateGraph(state_schema=MessagesState)
# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„å‡½æ•°
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}
# å®šä¹‰å›¾ä¸­çš„ï¼ˆå•ä¸ªï¼‰èŠ‚ç‚¹
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
# æ·»åŠ å†…å­˜æŒä¹…åŒ–
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª `config`ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨å¯è¿è¡Œå¯¹è±¡æ—¶ä¼ å…¥ã€‚
è¿™ä¸ªé…ç½®åŒ…å«ä¸€äº›ä¸ç›´æ¥å±äºè¾“å…¥çš„æ•°æ®ï¼Œä½†ä»ç„¶å¾ˆé‡è¦çš„ä¿¡æ¯ã€‚
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›åŒ…å«ä¸€ä¸ª `thread_id`ï¼ˆçº¿ç¨‹ IDï¼‰ï¼Œé…ç½®å¦‚ä¸‹ï¼š

```python
config = {"configurable": {"thread_id": "abc123"}}
```

è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨ä¸€ä¸ªåº”ç”¨ä¸­åŒæ—¶æ”¯æŒå¤šä¸ªä¼šè¯çº¿ç¨‹ï¼Œè¿™æ˜¯æ”¯æŒå¤šç”¨æˆ·èŠå¤©åº”ç”¨æ—¶çš„å¸¸è§éœ€æ±‚ã€‚

ç„¶åæˆ‘ä»¬å°±å¯ä»¥è°ƒç”¨è¿™ä¸ªåº”ç”¨ï¼š

```python
query = "Hi! I'm Bob."
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()  # è¾“å‡ºåŒ…å«äº†æ‰€æœ‰æ¶ˆæ¯çš„çŠ¶æ€
```

è¾“å‡ºç»“æœï¼š

```
================================== [1m Ai Message [0m ==================================
Hi Bob! How can I assist you today?
```

æ¥ç€æˆ‘ä»¬ç»§ç»­å¯¹è¯ï¼š

```python
query = "What's my name?"
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

è¾“å‡ºç»“æœï¼š

```
================================== [1m Ai Message [0m ==================================
Your name is Bob! How can I help you today, Bob?
```

å¾ˆå¥½ï¼ç°åœ¨æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººå·²ç»å¯ä»¥**è®°ä½æˆ‘ä»¬è¯´è¿‡çš„è¯**äº†ã€‚

å¦‚æœæˆ‘ä»¬ä¿®æ”¹ `config`ï¼Œå°†å…¶æŒ‡å‘ä¸€ä¸ªä¸åŒçš„ `thread_id`ï¼Œå°±èƒ½çœ‹åˆ°ä¸€ä¸ªâ€œå…¨æ–°çš„å¯¹è¯â€ï¼š

```python
config = {"configurable": {"thread_id": "abc234"}}
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

è¾“å‡ºç»“æœï¼š

```
================================== [1m Ai Message [0m ==================================
I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?
```

ä¸è¿‡æˆ‘ä»¬ä»ç„¶å¯ä»¥éšæ—¶å›åˆ°åŸæ¥çš„å¯¹è¯ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å°†å®ƒ**æŒä¹…åŒ–å­˜å‚¨åœ¨æ•°æ®åº“**ä¸­äº†ã€‚

```python
config = {"configurable": {"thread_id": "abc123"}}
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

```
==================================[1m Ai Message [0m==================================
Your name is Bob. What would you like to discuss today?
```

è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•æ”¯æŒä¸€ä¸ªå¯ä»¥ä¸å¤šä¸ªç”¨æˆ·è¿›è¡Œå¯¹è¯çš„èŠå¤©æœºå™¨äººï¼

---

ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªæ˜¯ç»™æ¨¡å‹æ·»åŠ äº†ä¸€ä¸ªç®€å•çš„æŒä¹…åŒ–å±‚ã€‚
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ  **prompt æ¨¡æ¿**ï¼Œè®©èŠå¤©æœºå™¨äººæ›´åŠ å¤æ‚å’Œä¸ªæ€§åŒ–ã€‚

---

## Prompt æ¨¡æ¿ï¼ˆPrompt templatesï¼‰

[Prompt Templates](https://langchain-doc.cn/concepts/prompt_templates) å¯ä»¥å°†åŸå§‹ç”¨æˆ·ä¿¡æ¯è½¬åŒ–ä¸º LLM å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒåŸå§‹çš„ç”¨æˆ·è¾“å…¥åªæ˜¯ä¸€ä¸ªæ¶ˆæ¯ï¼Œæˆ‘ä»¬æŠŠå®ƒä¼ é€’ç»™ LLMã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è®©å®ƒç¨å¾®å¤æ‚ä¸€äº›ï¼š
é¦–å…ˆï¼Œæ·»åŠ ä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯ï¼ˆsystem messageï¼‰ï¼ŒåŒ…å«ä¸€äº›è‡ªå®šä¹‰æŒ‡ä»¤ï¼ˆä½†ä»ç„¶ä»¥ messages ä½œä¸ºè¾“å…¥ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¾“å…¥ä¸­åŠ å…¥é™¤äº†æ¶ˆæ¯ä¹‹å¤–çš„å…¶ä»–å†…å®¹ã€‚

è¦æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ª `ChatPromptTemplate`ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `MessagesPlaceholder` æ¥ä¼ é€’æ‰€æœ‰æ¶ˆæ¯ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You talk like a pirate. Answer all questions to the best of your ability.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°åº”ç”¨ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡æ¿ï¼š

```python
workflow = StateGraph(state_schema=MessagesState)
def call_model(state: MessagesState):
    # highlight-start
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    # highlight-end
    return {"messages": response}
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

è°ƒç”¨åº”ç”¨çš„æ–¹æ³•ä¸ä¹‹å‰ç›¸åŒï¼š

```python
config = {"configurable": {"thread_id": "abc345"}}
query = "Hi! I'm Jim."
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

```
==================================[1m Ai Message [0m==================================
Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!
```

```python
query = "What is my name?"
input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

```
==================================[1m Ai Message [0m==================================
Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!
```

å¤ªæ£’äº†ï¼
ç°åœ¨è®©æˆ‘ä»¬è®© prompt æ¨¡æ¿ç¨å¾®å¤æ‚ä¸€äº›ã€‚å‡è®¾æ–°çš„ prompt æ¨¡æ¿å¦‚ä¸‹ï¼š

```python
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability in {language}.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```

æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨ prompt ä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ `language` è¾“å…¥ã€‚
æ­¤æ—¶ï¼Œæˆ‘ä»¬çš„åº”ç”¨æœ‰ä¸¤ä¸ªå‚æ•°â€”â€”è¾“å…¥çš„ `messages` å’Œ `language`ã€‚
æˆ‘ä»¬éœ€è¦æ›´æ–°åº”ç”¨çš„çŠ¶æ€ä»¥åæ˜ è¿™ä¸€å˜åŒ–ã€‚

```python
from typing import Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict
# highlight-next-line
class State(TypedDict):
    # highlight-next-line
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # highlight-next-line
    language: str
workflow = StateGraph(state_schema=State)
def call_model(state: State):
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    return {"messages": [response]}
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

---

```python
config = {"configurable": {"thread_id": "abc456"}}
query = "Hi! I'm Bob."
language = "Spanish"
input_messages = [HumanMessage(query)]
output = app.invoke(
    # highlight-next-line
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()
```

```
==================================[1m Ai Message [0m==================================
Â¡Hola, Bob! Â¿CÃ³mo puedo ayudarte hoy?
```

è¯·æ³¨æ„ï¼Œæ•´ä¸ªçŠ¶æ€éƒ½ä¼šè¢«æŒä¹…åŒ–ï¼Œå› æ­¤å¦‚æœä¸æƒ³æ›´æ”¹æŸäº›å‚æ•°ï¼ˆæ¯”å¦‚ `language`ï¼‰ï¼Œå¯ä»¥çœç•¥å®ƒä»¬ï¼š

```python
query = "What is my name?"
input_messages = [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages},
    config,
)
output["messages"][-1].pretty_print()
```

```
==================================[1m Ai Message [0m==================================
Tu nombre es Bob. Â¿Hay algo mÃ¡s en lo que pueda ayudarte?
```

è¦ç†è§£å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¯ä»¥æŸ¥çœ‹ [LangSmith trace](https://smith.langchain.com/public/15bd8589-005c-4812-b9b9-23e74ba4c3c6/r)ã€‚

---

## ç®¡ç†å¯¹è¯å†å²ï¼ˆManaging Conversation Historyï¼‰

æ„å»ºèŠå¤©æœºå™¨äººæ—¶ï¼Œä¸€ä¸ªé‡è¦æ¦‚å¿µæ˜¯å¦‚ä½•ç®¡ç†å¯¹è¯å†å²ã€‚å¦‚æœä¸åŠ ç®¡ç†ï¼Œæ¶ˆæ¯åˆ—è¡¨ä¼šæ— é™å¢é•¿ï¼Œå¹¶å¯èƒ½æº¢å‡º LLM çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰ã€‚å› æ­¤ï¼Œéœ€è¦å¢åŠ ä¸€ä¸ªæ­¥éª¤æ¥é™åˆ¶ä¼ å…¥æ¶ˆæ¯çš„å¤§å°ã€‚

**é‡è¦ï¼šä½ éœ€è¦åœ¨ prompt æ¨¡æ¿ä¹‹å‰ï¼Œä½†åœ¨ä»æ¶ˆæ¯å†å²ä¸­åŠ è½½å…ˆå‰æ¶ˆæ¯ä¹‹åæ‰§è¡Œè¿™ä¸ªæ­¥éª¤ã€‚**

æˆ‘ä»¬å¯ä»¥åœ¨ prompt ä¹‹å‰æ·»åŠ ä¸€ä¸ªæ­¥éª¤ï¼Œä¿®æ”¹ `messages` é”®ï¼Œç„¶åå°†è¿™ä¸ªæ–°é“¾åŒ…è£…åœ¨ Message History ç±»ä¸­ã€‚

LangChain æä¾›äº†ä¸€äº›å†…ç½®çš„è¾…åŠ©å·¥å…·æ¥ [ç®¡ç†æ¶ˆæ¯åˆ—è¡¨](../how_to/index.html#messages)ã€‚
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ [trim_messages](https://langchain-doc.cn/how_to/trim_messages/) æ¥å‡å°‘å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ•°é‡ã€‚è¯¥å·¥å…·å…è®¸ä½ æŒ‡å®šä¿ç•™çš„ token æ•°é‡ï¼Œä»¥åŠå…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚æ˜¯å¦æ€»æ˜¯ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ˜¯å¦å…è®¸éƒ¨åˆ†æ¶ˆæ¯ï¼š

```python
from langchain_core.messages import SystemMessage, trim_messages
trimmer = trim_messages(
    max_tokens=65,
    strategy="last",
    token_counter=model,
    include_system=True,
    allow_partial=False,
    start_on="human",
)
messages = [
    SystemMessage(content="you're a good assistant"),
    HumanMessage(content="hi! I'm bob"),
    AIMessage(content="hi!"),
    HumanMessage(content="I like vanilla ice cream"),
    AIMessage(content="nice"),
    HumanMessage(content="whats 2 + 2"),
    AIMessage(content="4"),
    HumanMessage(content="thanks"),
    AIMessage(content="no problem!"),
    HumanMessage(content="having fun?"),
    AIMessage(content="yes!"),
]
trimmer.invoke(messages)
```

```
[SystemMessage(content="you're a good assistant", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),
 AIMessage(content='4', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),
 AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),
 AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]
```

è¦åœ¨é“¾ä¸­ä½¿ç”¨å®ƒï¼Œåªéœ€åœ¨å°† `messages` è¾“å…¥ä¼ ç»™ prompt ä¹‹å‰è¿è¡Œ trimmerã€‚

```python
workflow = StateGraph(state_schema=State)
def call_model(state: State):
    print(f"Messages before trimming: {len(state['messages'])}")
    # highlight-start
    trimmed_messages = trimmer.invoke(state["messages"])
    print(f"Messages after trimming: {len(trimmed_messages)}")
    print("Remaining messages:")
    for msg in trimmed_messages:
        print(f"  {type(msg).__name__}: {msg.content}")
    prompt = prompt_template.invoke(
        {"messages": trimmed_messages, "language": state["language"]}
    )
    response = model.invoke(prompt)
    # highlight-end
    return {"messages": [response]}
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

---

ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬é—®æ¨¡å‹â€œæˆ‘çš„åå­—æ˜¯ä»€ä¹ˆâ€ï¼Œå®ƒå¯èƒ½ä¸çŸ¥é“ï¼Œå› ä¸ºæˆ‘ä»¬è£å‰ªäº†èŠå¤©å†å²ï¼ˆé€šè¿‡å°† trim ç­–ç•¥è®¾ä¸º `'last'`ï¼Œåªä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ä»¥ç¬¦åˆ `max_tokens`ï¼‰ï¼š

```python
config = {"configurable": {"thread_id": "abc567"}}
query = "What is my name?"
language = "English"
# highlight-next-line
input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()
```

```
Messages before trimming: 12
Messages after trimming: 8
Remaining messages:
  SystemMessage: you're a good assistant
  HumanMessage: whats 2 + 2
  AIMessage: 4
  HumanMessage: thanks
  AIMessage: no problem!
  HumanMessage: having fun?
  AIMessage: yes!
  HumanMessage: What is my name?
==================================[1m Ai Message [0m==================================
I don't know your name. If you'd like to share it, feel free!
```

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬è¯¢é—®æœ€è¿‘æ¶ˆæ¯ä¸­çš„ä¿¡æ¯ï¼Œå®ƒä»ç„¶ä¼šè®°å¾—ï¼š

```python
config = {"configurable": {"thread_id": "abc678"}}
query = "What math problem was asked?"
language = "English"
input_messages = messages + [HumanMessage(query)]
output = app.invoke(
    {"messages": input_messages, "language": language},
    config,
)
output["messages"][-1].pretty_print()
```

```
Messages before trimming: 12
Messages after trimming: 8
Remaining messages:
  SystemMessage: you're a good assistant
  HumanMessage: whats 2 + 2
  AIMessage: 4
  HumanMessage: thanks
  AIMessage: no problem!
  HumanMessage: having fun?
  AIMessage: yes!
  HumanMessage: What math problem was asked?
==================================[1m Ai Message [0m==================================
The math problem that was asked was "what's 2 + 2."
```

å¦‚æœæŸ¥çœ‹ LangSmithï¼Œä½ å¯ä»¥åœ¨ [LangSmith trace](https://smith.langchain.com/public/04402eaa-29e6-4bb1-aa91-885b730b6c21/r) ä¸­çœ‹åˆ°åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆã€‚

## æµå¼ä¼ è¾“ï¼ˆStreamingï¼‰

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰ä¸€ä¸ªå¯ä»¥è¿è¡Œçš„èŠå¤©æœºå™¨äººäº†ã€‚ç„¶è€Œï¼Œå¯¹äºèŠå¤©æœºå™¨äººåº”ç”¨æ¥è¯´ï¼Œæœ‰ä¸€ä¸ª**éå¸¸é‡è¦çš„ç”¨æˆ·ä½“éªŒï¼ˆUXï¼‰è€ƒè™‘**ï¼šé‚£å°±æ˜¯**æµå¼è¾“å‡ºï¼ˆstreamingï¼‰**ã€‚

å› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“åº”å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´æ‰èƒ½ç”Ÿæˆï¼Œä¸ºäº†æå‡ç”¨æˆ·ä½“éªŒï¼Œå¤§å¤šæ•°èŠå¤©ç±»åº”ç”¨ä¼šé‡‡ç”¨â€œè¾¹ç”Ÿæˆè¾¹è¾“å‡ºâ€çš„æ–¹å¼ï¼Œå³å½“æ¨¡å‹ç”Ÿæˆæ¯ä¸ª tokenï¼ˆæ–‡å­—ç‰‡æ®µï¼‰æ—¶ï¼Œå°±ç«‹å³è¿”å›ç»™ç”¨æˆ·æ˜¾ç¤ºã€‚è¿™æ ·ï¼Œç”¨æˆ·å¯ä»¥**å®æ—¶çœ‹åˆ°æ¨¡å‹æ€è€ƒçš„è¿‡ç¨‹**ã€‚

åœ¨ LangChain ä¸­ï¼Œè¿™ç§æµå¼è¾“å‡ºå®ç°èµ·æ¥éå¸¸ç®€å•â€”â€”å‡ ä¹ä¸æ™®é€šè°ƒç”¨ä¸€æ ·ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è®©èŠå¤©æœºå™¨äººæ”¯æŒæµå¼ä¼ è¾“ã€‚

```python
from langchain_core.messages import HumanMessage
config = {"configurable": {"thread_id": "streaming_123"}}
query = "Write a short poem about the ocean."
input_messages = [HumanMessage(query)]
```

### æ™®é€šè°ƒç”¨ï¼ˆéæµå¼ï¼‰

```python
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

æ¨¡å‹ä¼šç­‰å¾…å…¨éƒ¨ç”Ÿæˆå®Œæ¯•åä¸€æ¬¡æ€§è¿”å›ç»“æœã€‚

---

### ä½¿ç”¨æµå¼è¾“å‡º

å¦‚æœæˆ‘ä»¬æƒ³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥æ¥æ”¶è¾“å‡ºï¼Œåªéœ€è¦è°ƒç”¨ `.stream()` æ–¹æ³•ï¼š

```python
for chunk in app.stream({"messages": input_messages}, config):
    print(chunk)
```

ä½ ä¼šçœ‹åˆ°æ¨¡å‹é€æ­¥è¾“å‡ºæ¯ä¸€éƒ¨åˆ†å†…å®¹ï¼Œæ¯”å¦‚ï¼š

```
{'messages': [AIMessageChunk(content='The ')]}
{'messages': [AIMessageChunk(content='ocean ')]}
{'messages': [AIMessageChunk(content='is ')]}
...
```

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ å¯ä»¥å°†è¿™äº›è¾“å‡ºå®æ—¶æ˜¾ç¤ºåˆ°å‰ç«¯ç•Œé¢ï¼Œä»è€Œè®©ç”¨æˆ·è·å¾—â€œå®æ—¶å›å¤â€çš„ä½“éªŒï¼Œå°±åƒ ChatGPT ä¸€æ ·ã€‚

---

### å¼‚æ­¥æµå¼æ”¯æŒ

å¦‚æœä½ æ­£åœ¨ä½¿ç”¨å¼‚æ­¥æ¡†æ¶ï¼ˆå¦‚ FastAPI æˆ– Jupyter Notebook çš„å¼‚æ­¥ç¯å¢ƒï¼‰ï¼ŒLangChain ä¹Ÿæä¾›äº†å¼‚æ­¥ç‰ˆæœ¬çš„æµå¼è°ƒç”¨ï¼š

```python
async for chunk in app.astream({"messages": input_messages}, config):
    print(chunk)
```

è¿™åœ¨é«˜å¹¶å‘åœºæ™¯æˆ–å®æ—¶äº¤äº’å¼åº”ç”¨ä¸­éå¸¸æœ‰ç”¨ã€‚

---

## æ€»ç»“

æˆ‘ä»¬åˆšåˆšä¸€æ­¥æ­¥æ„å»ºäº†ä¸€ä¸ªå¯ä»¥**è®°å¿†ä¸Šä¸‹æ–‡ã€æ”¯æŒå¤šè½®å¯¹è¯ã€å…·æœ‰å¤šçº¿ç¨‹æŒä¹…åŒ–**å¹¶ä¸”**æ”¯æŒæµå¼å“åº”**çš„èŠå¤©æœºå™¨äººã€‚

ä½ ç°åœ¨å·²ç»æŒæ¡äº†ï¼š

âœ… å¦‚ä½•ä½¿ç”¨ LangChain çš„ ChatModel æ„å»ºå¯¹è¯
âœ… å¦‚ä½•é€šè¿‡ LangGraph æ·»åŠ æŒä¹…åŒ–å†…å­˜
âœ… å¦‚ä½•ä½¿ç”¨ Prompt Template å®šä¹‰ä¸ªæ€§åŒ–é£æ ¼
âœ… å¦‚ä½•ç®¡ç†ä¼šè¯å†å²é˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡º
âœ… å¦‚ä½•ä½¿ç”¨ Streaming æå‡èŠå¤©ä½“éªŒ

è¿™äº›æ˜¯æ„å»ºä»»ä½•æ™ºèƒ½èŠå¤©ç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œä½ å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•ï¼Œæ¯”å¦‚ï¼š

*   é›†æˆå¤–éƒ¨æ•°æ®åº“ï¼ˆPostgresã€Redisï¼‰ä¿å­˜ä¼šè¯
*   ä½¿ç”¨ LangGraph çš„ **branching & conditional logic**ï¼ˆæ¡ä»¶é€»è¾‘åˆ†æ”¯ï¼‰
*   æ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆTool callingï¼‰è®©æœºå™¨äººæ‰§è¡Œæ“ä½œ
*   æ„å»ºå¤šè§’è‰²å¯¹è¯ç³»ç»Ÿï¼ˆMulti-agent systemsï¼‰

---

### æ›´å¤šèµ„æºï¼š

*   ğŸ“˜ [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
*   ğŸ§  [LangChain Chat Models æ¦‚å¿µ](https://langchain-doc.cn/concepts/chat_models)
*   ğŸ§© [LangGraph Memoryï¼ˆè®°å¿†ç³»ç»Ÿï¼‰æ¦‚å¿µ](https://langchain-ai.github.io/langgraph/concepts/persistence/)
*   ğŸ§­ [LangSmith è°ƒè¯•ä¸å¯è§†åŒ–å¹³å°](https://smith.langchain.com)